---
title: "Exploratory Data Analysis"
output: md_document
editor_options: 
  chunk_output_type: console
---

Our data is a combination of two different datasets. Census data from 2010 was taken from the City of Portland website (https://www.portlandoregon.gov/civic/56897) and data about police response time from 2012 was taken from the Portland Police Bureau (https://www.portlandoregon.gov/police/76454.). Rather than using total response time, we are using the time spent in the queue, that is, the amount of time a caller waited in the police dispatch queue before an officer was dispatched to them. The total response time includes travel time once the officer was dispatched. As there is no record of how far the officer had to travel, there was no way to standardize the total response time, so we used the time spent in the queue as a more standardized response time.

The unit of observation in this combined dataset is a call to the Portland Police Bureau (PPB). The number of observations is 194,044. To predict the time in queue of the PPB we are looking at priority of the crime as rated by the PPB, racial makeup of the neighborhood (in percentage of white population), population density, and average income in the neighborhood.

Location data is majority of data that is missing in observations with missing data. This is addressed by PPB on the website. If "the incident occurred outside of the boundaries of the Portland neighborhoods or at a location that could not be assigned to a specific address in the system (e.g., Portland, near Washington Park, on the streetcar, etc.)" (PPB) there will be no data for location. 

Population and economic data came from Portland Monthly: (https://www.pdxmonthly.com/articles/2016/4/1/real-estate-2016-the-city)

### Time in Queue for PPB calls
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggfortify)
library(glmnet)
library(MLmetrics)
library(tree)
library(caret)
library(randomForest)
library(gbm)
library(caret)
library(bst)
library(caretEnsemble)
calls <- read.csv("https://raw.githubusercontent.com/stat-learning/group-1/master/calls.csv")
```


```{r dummies}
priority_dummy <- NULL
priority_dummy[calls$Priority == "Low"] = 0
priority_dummy[calls$Priority == "Medium"] = 0.5
priority_dummy[calls$Priority == "High"] = 1

category_dummy <- NULL
category_dummy[calls$FinalCallGroup == "Traffic"] = 0
category_dummy[calls$FinalCallGroup == "Disorder"] = 0
category_dummy[calls$FinalCallGroup == "Community Policing"] = 0
category_dummy[calls$FinalCallGroup == "Assist"] = 0
category_dummy[calls$FinalCallGroup == "Other"] = 0
category_dummy[calls$FinalCallGroup == "Alarm"] = 0
category_dummy[calls$FinalCallGroup == "Civil"] = 0
category_dummy[calls$FinalCallGroup == "Crime"] = 1

calls_dummy <- mutate(calls, priority_dummy)
calls_dummy <- mutate(calls_dummy, category_dummy)

```

```{r Call Frequency}

ggplot(data=calls, aes(x=TimeInQueue_sec)) + 
  geom_histogram(binwidth=500) +
  labs(x = "Time in Queue", y="Frequency" , title="Frequency of Time in Queue", caption="Number of Observations = 154247, Dashed Line represents Mean of the Call Times") +
  geom_vline(aes(xintercept=mean(TimeInQueue_sec)),
             color="blue", linetype="dashed", size=1)

summary(calls$TimeInQueue_sec)
```


```{R Bivariate Predictor Race}
ggplot(data = calls, mapping = aes(x = Pct.White,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1)+
labs(x = "Percentage of White Residents", y = "Time Spent in Queue")

#same as above but faceted by priority
ggplot(data = calls, mapping = aes(x = Pct.White,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1) +
facet_wrap(~Priority, ncol = 3) +
labs(x = "Percentage of White Residents", y = "Time Spent in Queue")

```


```{R Bivariate Predictor Population Density}
#population density represented by its z-score
ggplot(data = calls, mapping = aes(x = Std_pop,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1)+
labs(x = "Population Density", y = "Time Spent in Queue")

#same as above but faceted by priority
ggplot(data = calls, mapping = aes(x = Std_pop,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1) +
facet_wrap(~Priority, ncol = 3) +
labs(x = "Population Density", y = "Time Spent in Queue")

```


```{R Predictor Mean Income}
ggplot(data = calls, mapping = aes(x = Income.Std,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1)+
labs(x = "Mean Income", y = "Time Spent in Queue")

#same as above but faceted by priority
ggplot(data = calls, mapping = aes(x = Income.Std,
                                   y = TimeInQueue_sec)) +
geom_point(alpha = 0.1) +
facet_wrap(~Priority, ncol = 3) +
labs(x = "Mean Income", y = "Time Spent in Queue")

```


```{r PCA}
#removing the categorical variables
numericalCalls<-calls[-c(2:4)]
#chaning the priorities into numerical values.
numericalCalls<-numericalCalls%>%mutate(Priority=case_when(
  Priority=="High" ~1,
  Priority=="Medium" ~2,
  Priority=="Low" ~3
))

numericalCalls %>% drop_na()
income_pos <- (numericalCalls$Income.Std + 1.353877)
log_inc <- log(income_pos)
log_time <- log(numericalCalls$TimeInQueue_sec)

numericalCalls<- mutate(numericalCalls, logTime=log_time, logIncome=log_inc)
numericalCalls$logTime[which(!is.finite(numericalCalls$logTime))] <- 0

# creating PCA model
calls.PCA<-prcomp(numericalCalls, center=TRUE, scale.= TRUE)
summary(calls.PCA)
str(calls.PCA)

#plotting PCA
autoplot(calls.PCA, alpha=0.1, loadings=TRUE, loadings.label=TRUE)
screeplot(calls.PCA, type="lines")

#based on the scree plot, we should use 3 PC's. 
PC1<-calls.PCA$x[,1]
PC2<-calls.PCA$x[,2]
PC3<-calls.PCA$x[,3]
```

We tried a basic non-crossvalidated linear model just to see how the skew of the 
variables would impact the diagnostic graphs. Upon seeing the non-random residuals 
and highly non-linear QQ plot, we applied log transformations to some of the variables. 
The linear model using the transformed variables still had problems in the diagnostic 
graphs, but they were improved from the non-transformed model. When we built a 
cross-validated linear model, we decided to use these transformed variables.

```{r linear regression}
#without tranformed variables
lm_base <- lm(TimeInQueue_sec ~ Pct.White +
            Std_pop +
            Income.Std +
            priority_dummy +
            category_dummy,
          data = calls_dummy)
summary(lm_base)
#Checking the residual graphs
plot(lm_base)

```

```{r transforming variables}

income_positive <- (calls_dummy$Income.Std + 1.353877)
log_income <- log(income_positive)
hist(log_income)

log_queue <- log(calls_dummy$TimeInQueue_sec)
hist(log_queue)

calls_dummy <- add_column(calls_dummy, log_income, .after = 9)
calls_dummy <- add_column(calls_dummy, log_queue, .after = 10)

#remove negative infinity variables
calls_dummy <- calls_dummy[!is.infinite(log_queue),]

```

```{r transformed model}
# with transformed variables
lm_transformed <- lm(log_queue ~ Pct.White +
            log_income +
            priority_dummy +
            category_dummy,
          data = calls_dummy)
summary(lm_transformed)
#Checking the residual graphs - these look a lot better
plot(lm_transformed)
```


We tested six different models: three regression-based models and three algorithmic 
models. Using 10-fold cross-validation, we fit multiple linear regression, ridge 
regression, lasso, bagged trees, random forest, and boosted forest models. The 
caretEnsemble package tested the parameters for all of these models and chose the best 
performing model of each type. Since many of our variables were skewed, even after 
transformation, we anticipated the algorithmic approach to be more appropriate. 
However, since we wanted to engage in inference regarding time in the queue, linear-based 
models would provide more interpretable information.

```{r caret, cache = TRUE}
set.seed(23489)
train_index <- sample(1:nrow(calls_dummy), 0.9 * nrow(calls_dummy))
calls_train <- calls_dummy[train_index, ]
calls_test <- calls_dummy[-train_index, ]

preds <- c(7, 9:10, 14:15)
train_preds <- as.matrix(calls_train[,preds])
test_preds <- as.matrix(calls_test[,preds])

fit_control <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10,
                           allowParallel = TRUE)

model_list <- caretList(log_queue ~ Pct.White + log_income + category_dummy + priority_dummy,
                        trControl = fit_control,
                        data=calls_train,
                        methodList = c("lm", "glmnet", "ranger", 
                                       "gbm", "treebag"),
                        tuneList = NULL,
                        continue_on_fail = FALSE)

best_lm <- model_list$lm$finalModel

best_glmnet <- model_list$glmnet$finalModel

best_rf <- model_list$ranger$finalModel

best_bagged_trees <- model_list$treebag$finalModel

best_boosted_forest <- model_list$gbm$finalModel

options(digits = 3)
model_results <- data.frame(
 LM = min(model_list$lm$results$RMSE),
 GLMNET = min(model_list$glmnet$results$RMSE),
 RANGER = min(model_list$ranger$results$RMSE),
 TREEBAG = min(model_list$treebag$results$RMSE),
 GBM = min(model_list$gbm$results$RMSE)
 )

print(model_results)
```

Our dataset was large enough that we could tune the models through 10-fold CV on 
training set and then test the models into a test set. The test MSEs for the best 
model of each type is shown below.

```{r test MSEs}

lm_yhats <- predict(model_list$lm, newdata = calls_test)
best_lm_mse <- MSE(lm_yhats, calls_test$log_queue)


glmnet_yhats <- predict(model_list$glmnet,
                        newdata = calls_test)
best_glmnet_mse <- MSE(glmnet_yhats, calls_test$log_queue)


rf_yhats <- predict(model_list$ranger, newdata = calls_test)
best_rf_mse <- MSE(rf_yhats, calls_test$log_queue)

bagged_yhats <- predict(model_list$treebag,newdata = calls_test)
best_bagged_mse <- MSE(bagged_yhats, calls_test$log_queue)
  
boosted_yhats <- predict(model_list$gbm, newdata = calls_test)
best_boosted_mse <- MSE(boosted_yhats, calls_test$log_queue)


options(digits = 3)
test_MSE <- data.frame(
 LM = best_lm_mse,
 GLMNET = best_glmnet_mse,
 RANGER = best_rf_mse,
 TREEBAG = best_bagged_mse,
 GBM = best_boosted_mse
 )

print(test_MSE)
```
The boosted random forest model has the lowest test MSE. We used variable importance
to infer how much each variable was impacting the timein the queue. Unsurprisingly, 
priority and category were overwhelmingly the most importance.
```{r variable importance}
summary(best_boosted_forest)
```
We tried a boosted random forest model without priority and then without priority 
and category to compare the race and income variables. Interestingly, when we 
removed priority, income switched places with race as the more important of the 
two variables, indicating that priority may have been capturing some of the importance 
of income in the original model.
```{r without priority}

no_priority <- caretList(log_queue ~ Pct.White + log_income + category_dummy,
                        trControl = fit_control,
                        data=calls_train,
                        methodList = c("gbm"),
                        tuneList = NULL,
                        continue_on_fail = FALSE)

summary(no_priority$gbm$finalModel)

no_p_or_c <- caretList(log_queue ~ Pct.White + log_income,
                        trControl = fit_control,
                        data=calls_train,
                        methodList = c("gbm"),
                        tuneList = NULL,
                        continue_on_fail = FALSE)
summary(no_p_or_c$gbm$finalModel)
```

Looking at our best linear regression and penalized regression models, we get more 
insights for inference. In the non-penalized model, all of the variables are 
significant at the 0.001 significance level. However, the coefficient for priority 
is much, much higher than the other coefficients. The penalized regression reduced 
the coefficients of all of the variables but did not make any of them 
zero, so we cannot fully discount any of the four variables.

```{r linear summaries}
summary(best_lm)

coef(best_glmnet, 0.00254)
```